= Observability
include::_attributes.adoc[]

[#monitoring]
== Monitoring with Grafana

Out of the box, you get monitoring via Prometheus and Grafana. 

NOTE: You may want to push some load onto the deployed applications in order to see some metrics.  

Run the following command to produce load.

[source,bash,subs="+macros,+attributes"]
----
./scripts/run.sh http://istio-ingressgateway-istio-system.{appdomain}/{path}
----

[source,bash,subs="+macros,+attributes"]
----
ifdef::workshop[]
open http://grafana-istio-system.{appdomain}/d/TSEY6jLmk/istio-galley-dashboard?orgId=1
or
firefox http://grafana-istio-system.{appdomain}/d/TSEY6jLmk/istio-galley-dashboard?orgId=1
endif::workshop[]

ifndef::workshop[]
open "$(minishift openshift service grafana -u -n istio-system)/d/TSEY6jLmk/istio-galley-dashboard?refresh=5s&orgId=1"
or
firefox "$(minishift openshift service grafana -u -n istio-system)/d/TSEY6jLmk/istio-galley-dashboard?refresh=5s&orgId=1"
endif::workshop[]
----

image:grafana1.png[Grafana 1]

[source,bash,subs="+macros,+attributes"]
----
ifdef::workshop[]
http://grafana-istio-system.{appdomain}/d/UbsSZTDik/istio-workload-dashboard?refresh=5s&orgId=1"
endif::workshop[]

ifndef::workshop[]
open "$(minishift openshift service grafana -u -n istio-system)/d/UbsSZTDik/istio-workload-dashboard?orgId=1&refresh=10s"
or
firefox "$(minishift openshift service grafana -u -n istio-system)/d/UbsSZTDik/istio-workload-dashboard?orgId=1&refresh=10s"
endif::workshop[]
----

to check the "Workload of the services"

image:grafana2.png[Grafana 2]

[#prometheus]
== Prometheus

Explore Prometheus:

[source,bash,subs="+macros,+attributes"]
----
ifdef::workshop[]
open http://prometheus-istio-system.{appdomain}/graph?g0.range_input=1m&g0.stacked=1&g0.expr=&g0.tab=0"
or
firefox http://prometheus-istio-system.{appdomain}/graph?g0.range_input=1m&g0.stacked=1&g0.expr=&g0.tab=0"
endif::workshop[]

ifndef::workshop[]
open "$(minishift openshift service prometheus -u -n istio-system)/graph?g0.range_input=1m&g0.stacked=1&g0.expr=&g0.tab=0"
or
firefox "$(minishift openshift service prometheus -u -n istio-system)/graph?g0.range_input=1m&g0.stacked=1&g0.expr=&g0.tab=0"
endif::workshop[]
----


// Custom Metrics removed from Workshop because it requires access to istio-system.
ifndef::workshop[] 
[#custommetrics]
=== Custom Metrics

Istio also allows you to specify custom metrics which can be seen inside of the Prometheus dashboard

Add the custom metric and rule. First make sure you are in the "istio-tutorial" directory and then

[source,bash,subs="+macros,+attributes"]
----
kubectl create -f link:{github-repo}/{istiofiles-dir}/recommendation_requestcount.yml[istiofiles/recommendation_requestcount.yml] -n istio-system
----


Then run several requests through the system

[source,bash,subs="+macros,+attributes"]
----
while true; do curl istio-ingressgateway-istio-system.{appdomain}/{path}; sleep .5;  done
----

In the Prometheus dashboard, add the following

[source,bash,subs="+macros,+attributes"]
----
istio_requests_total{destination_service="recommendation.tutorial.svc.cluster.local"}
----

and select `Execute`

image:prometheus_custom_metric.png[alt text]


NOTE: You may have to refresh the browser for the Prometheus graph to update. And you may wish to make the interval 5m (5 minutes) as seen in the screenshot above.
endif::workshop[]


[#containermemory]
=== Containers memory

Istio allows also to see the RSS memory consumed by the containers.

In the Prometheus dashboard, add the following

[source,bash,subs="+macros,+attributes"]
----
container_memory_rss{namespace="tutorial{namespace-suffix}",container_name =~ "customer|preference|recommendation"}
----

and select `Execute`

image:prometheus-memory.png[Prometheus memory]


[#tracing]
== Tracing

Distributed Tracing involves propagating the tracing context from service to service, usually done by sending certain incoming HTTP headers downstream to outbound requests. For services embedding a http://opentracing.io/[OpenTracing] framework instrumentations such as https://github.com/opentracing-contrib/java-spring-cloud[opentracing-spring-cloud], this might be transparent. For services that are not embedding OpenTracing libraries, this context propagation needs to be done manually.

As OpenTracing is "just" an instrumentation library, a concrete tracer is required in order to actually capture the tracing data and report it to a remote server. Our `customer` and `preference` services ship with http://jaegertracing.io/[Jaeger] as the concrete tracer. the Istio platform automatically sends collected tracing data to Jaeger, so that we are able to see a trace involving all three services, even if our `recommendation` service is not aware of OpenTracing or Jaeger at all.

Our `customer` and `preference` services are using the https://github.com/jaegertracing/jaeger-client-java/tree/master/jaeger-tracerresolver[`TracerResolver`] facility from OpenTracing, so that the concrete tracer can be loaded automatically without our code having a hard dependency on Jaeger. Given that the Jaeger tracer can be configured via environment variables, we don't need to do anything in order to get a properly configured Jaeger tracer ready and registered with OpenTracing. That said, there are cases where it's appropriate to manually configure a tracer. Refer to the Jaeger documentation for more information on how to do that.

Let's open the Jaeger console, select `customer` from the list of services and click `Find Traces`

[source,bash,subs="+macros,+attributes"]
----
ifdef::workshop[]
open https://jaeger-query-istio-system.{appdomain}/
or
firefox https://jaeger-query-istio-system.{appdomain}/
endif::workshop[]

ifndef::workshop[]
minishift openshift service tracing -n istio-system --in-browser
endif::workshop[]
----

image:jaegerUI.png[Trace as seen in Jaeger]
